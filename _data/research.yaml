categories:

  - data-filter: humanoid whole-body-control
    category-name: humanoid wbc


projects:

  - title: Learning Versatile Motion Skills for Humanoid Whole-Body Control
    system-name: KungfuBot2
    gif: assets/img/kungfubot2.gif
    conference: In submission 
    conference-web:
    status:
    authors: <u>Jinrui Han</u>, Weiji Xie, Jiakun Zheng, Jiyuan Shi, Weinan Zhang, Ting Xiao, Chenjia Bai†
    pdf: https://arxiv.org/abs/2509.16638
    code: https://github.com/TeleHuman/PBHC
    demo: https://kungfubot2-humanoid.github.io/
    # slides: https://mkhangg.com/assets/slides/iros24b_slides.pdf
    # talk: https://youtu.be/vQZMQApcTCY
    # poster: https://mkhangg.com/assets/posters/iros24b_poster.pdf
    abstract-less: Learning versatile whole-body skills by tracking various human motions is a fundamental step toward general-purpose humanoid robots. This task is particularly challenging because a single policy must master a broad repertoire of motion skills while ensuring stability over long-horizon sequences. To this end, we present VMS, 
    abstract-more: a unified whole-body controller that enables humanoid robots to learn diverse and dynamic behaviors within a single policy. Our framework integrates a hybrid tracking objective that balances local motion fidelity with global trajectory consistency, and an Orthogonal Mixture-of-Experts (OMoE) architecture that encourages skill specialization while enhancing generalization across motions. A segment-level tracking reward is further introduced to relax rigid step-wise matching, enhancing robustness when handling global displacements and transient inaccuracies. We validate VMS extensively in both simulation and real-world experiments, demonstrating accurate imitation of dynamic skills, stable performance over minute-long sequences, and strong generalization to unseen motions. These results highlight the potential of VMS as a scalable foundation for versatile humanoid whole-body control
    tag: humanoid wbc
    category: humanoid wbc

  - title: Physics-Based Humanoid Whole-Body Control for Learning Highly-Dynamic Skills
    system-name: KungfuBot
    gif: assets/img/kungfubot.png
    conference: NeurIPS 2025 (San Diego, United States)
    conference-web: https://neurips.cc/
    status:
    authors: Weiji Xie*, <u>Jinrui Han*</u>, Jiakun Zheng*, Huanyu Li, Xinzhe Liu, Jiyuan Shi, Weinan Zhang, Chenjia Bai†, Xuelong Li.
    pdf: https://arxiv.org/abs/2506.12851
    code: https://github.com/TeleHuman/PBHC
    demo: https://kungfu-bot.github.io/
    # slides: https://mkhangg.com/assets/slides/iros24a_slides.pdf
    talk: https://www.openrobot.me/talk
    # poster: https://mkhangg.com/assets/posters/iros24a_poster.pdf
    abstract-less: Humanoid robots are promising to acquire various skills by imitating human behaviors. However, existing algorithms are only capable of tracking smooth, low-speed human motions, even with delicate reward and curriculum design. This paper presents a physics-based humanoid control framework, 
    abstract-more: aiming to master highly-dynamic human behaviors such as Kungfu and dancing through multi-steps motion processing and adaptive motion tracking. For motion processing, we design a pipeline to extract, filter out, correct, and retarget motions, while ensuring compliance with physical constraints to the maximum extent. For motion imitation, we formulate a bi-level optimization problem to dynamically adjust the tracking accuracy tolerance based on the current tracking error, creating an adaptive curriculum mechanism. We further construct an asymmetric actor-critic framework for policy training. In experiments, we train whole-body control policies to imitate a set of highly-dynamic motions. Our method achieves significantly lower tracking errors than existing approaches and is successfully deployed on the Unitree G1 robot, demonstrating stable and expressive behaviors. 
    tag: humanoid wbc
    category: humanoid wbc
